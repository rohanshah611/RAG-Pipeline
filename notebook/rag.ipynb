{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = base_dir = os.getcwd()  \n",
    "parent_dir = os.path.dirname(base_dir)  \n",
    "\n",
    "\n",
    "file_path = os.path.join(parent_dir, \"artifacts\", \"research_paper_yolo.pdf\")\n",
    "output_path = os.path.join(parent_dir, \"artifacts\", \"images\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\",\"Table\"],   # Add 'Table' to list to extract image of tables\n",
    "    image_output_dir_path= output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=5000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=4000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<unstructured.documents.elements.CompositeElement object at 0x320754a40>, <unstructured.documents.elements.CompositeElement object at 0x320754260>, <unstructured.documents.elements.CompositeElement object at 0x320757fe0>, <unstructured.documents.elements.CompositeElement object at 0x320754080>, <unstructured.documents.elements.CompositeElement object at 0x320777110>, <unstructured.documents.elements.CompositeElement object at 0x31f64c290>, <unstructured.documents.elements.CompositeElement object at 0x3203cd8e0>, <unstructured.documents.elements.CompositeElement object at 0x30cfb9d60>, <unstructured.documents.elements.CompositeElement object at 0x3357d6060>, <unstructured.documents.elements.CompositeElement object at 0x320754410>, <unstructured.documents.elements.CompositeElement object at 0x31f389cd0>, <unstructured.documents.elements.CompositeElement object at 0x1744c4c20>, <unstructured.documents.elements.CompositeElement object at 0x17ccfb080>, <unstructured.documents.elements.CompositeElement object at 0x3330834d0>]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'rag-demo' already exists and is ready.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Create a new index with chosen dimension and metric\n",
    "index_name = \"rag-demo\"\n",
    "dimension = 1536  # Dimension should match your embedding model\n",
    "existing_indexes = pc.list_indexes().names()\n",
    "if index_name not in existing_indexes:\n",
    "    index = pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(f\"Index '{index_name}' created!\")\n",
    "else:\n",
    "    # Connect to existing index\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Index '{index_name}' already exists and is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if \"CompositeElement\" in str(type(chunk)):\n",
    "        chunk_elements = chunk.metadata.orig_elements\n",
    "        \n",
    "        # Combine all elements in this chunk into a single string\n",
    "        combined_content = \"\"\n",
    "        for element in chunk_elements:\n",
    "            if \"Text\" in str(type(element)):\n",
    "                combined_content += element.text + \"\\n\"\n",
    "            elif \"Table\" in str(type(element)):\n",
    "                combined_content += element.metadata.text_as_html + \"\\n\"\n",
    "            #elif \"Image\" in str(type(element)):\n",
    "                # Optional: store a placeholder or base64 if needed\n",
    "            #    combined_content += \"[IMAGE]\\n\"\n",
    "\n",
    "        # Generate a single embedding for the entire chunk\n",
    "        embedded_chunk = embeddings.embed_documents([combined_content])[0]\n",
    "\n",
    "        # Upsert into Pinecone with one ID per chunk\n",
    "        chunk_id = f\"chunk-{i}\"\n",
    "        index.upsert([{\n",
    "            \"id\": chunk_id,\n",
    "            \"values\": embedded_chunk,\n",
    "            \"metadata\": {\n",
    "                \"chunk_id\": i,\n",
    "                \"type\": \"composite\",\n",
    "                \"content\": combined_content,\n",
    "                \"text\": combined_content\n",
    "            }\n",
    "        }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: chunk-12\n",
      "Content: This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\n",
      "There is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\n",
      "This research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\n",
      "Author name / Procedia Computer Science 00 (2017) 000–000\n",
      "Jiang et al./ Procedia Computer Science 00 (2017) 000–000\n",
      "yolo v2 algorithm\n",
      "yolo v2\n",
      "yolo v3 object\n",
      "yolo v3 code\n",
      "References\n",
      "tensorflow\n",
      "detection\n",
      "Table 3. Top ten queries for each YOLO version (V4 and V5)\n",
      "YOLO V4\n",
      "YOLO V5\n",
      "GOOGLE\n",
      "YOUTUBE\n",
      "GOOGLE\n",
      "YOUTUBE\n",
      "yolo v4\n",
      "yolo v4\n",
      "yolo v5\n",
      "yolo v5\n",
      "yolo v4 alexeyab\n",
      "yolo v4 tutorial\n",
      "yolo v5 github\n",
      "yolo v5 vs v4\n",
      "yolo v4 github\n",
      "yolo v4 demo\n",
      "yolo v5 paper\n",
      "yolo v5 tutorial\n",
      "yolo v4 pytorch\n",
      "yolo v4 video\n",
      "yolo v5 tutorial\n",
      "yolo v5 object\n",
      "detection\n",
      "yolo v4 tiny\n",
      "yolo v4 colab\n",
      "yolo v5 vs v4\n",
      "yolo v5 colab\n",
      "yolo v4 vs v5\n",
      "yolo v4 tiny\n",
      "yolo v5\n",
      "yolo v5 demo\n",
      "architecture\n",
      "yolo v4\n",
      "yolo v4 google\n",
      "yolo v5 darknet\n",
      "yolo v5 video\n",
      "tensorflow\n",
      "colab\n",
      "yolo v4 tutorial\n",
      "yolo v4 object\n",
      "yolo v5\n",
      "yolo v5 pytorch\n",
      "detection\n",
      "tensorflow\n",
      "yolo v4 python\n",
      "yolo v4 training\n",
      "yolo v5\n",
      "yolo v5 paper\n",
      "tensorflow\n",
      "github\n",
      "yolo v4 training\n",
      "yolo v4\n",
      "yolo v5 tensorrt\n",
      "yolo v5\n",
      "6(2).\n",
      "tensorflow\n",
      "architecture\n",
      "\n",
      "Score: 0.59141165\n",
      "Metadata: {'chunk_id': 12.0, 'content': 'This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\\nThere is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\\nThis research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nyolo v2 algorithm\\nyolo v2\\nyolo v3 object\\nyolo v3 code\\nReferences\\ntensorflow\\ndetection\\nTable 3. Top ten queries for each YOLO version (V4 and V5)\\nYOLO V4\\nYOLO V5\\nGOOGLE\\nYOUTUBE\\nGOOGLE\\nYOUTUBE\\nyolo v4\\nyolo v4\\nyolo v5\\nyolo v5\\nyolo v4 alexeyab\\nyolo v4 tutorial\\nyolo v5 github\\nyolo v5 vs v4\\nyolo v4 github\\nyolo v4 demo\\nyolo v5 paper\\nyolo v5 tutorial\\nyolo v4 pytorch\\nyolo v4 video\\nyolo v5 tutorial\\nyolo v5 object\\ndetection\\nyolo v4 tiny\\nyolo v4 colab\\nyolo v5 vs v4\\nyolo v5 colab\\nyolo v4 vs v5\\nyolo v4 tiny\\nyolo v5\\nyolo v5 demo\\narchitecture\\nyolo v4\\nyolo v4 google\\nyolo v5 darknet\\nyolo v5 video\\ntensorflow\\ncolab\\nyolo v4 tutorial\\nyolo v4 object\\nyolo v5\\nyolo v5 pytorch\\ndetection\\ntensorflow\\nyolo v4 python\\nyolo v4 training\\nyolo v5\\nyolo v5 paper\\ntensorflow\\ngithub\\nyolo v4 training\\nyolo v4\\nyolo v5 tensorrt\\nyolo v5\\n6(2).\\ntensorflow\\narchitecture\\n', 'text': 'This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\\nThere is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\\nThis research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nyolo v2 algorithm\\nyolo v2\\nyolo v3 object\\nyolo v3 code\\nReferences\\ntensorflow\\ndetection\\nTable 3. Top ten queries for each YOLO version (V4 and V5)\\nYOLO V4\\nYOLO V5\\nGOOGLE\\nYOUTUBE\\nGOOGLE\\nYOUTUBE\\nyolo v4\\nyolo v4\\nyolo v5\\nyolo v5\\nyolo v4 alexeyab\\nyolo v4 tutorial\\nyolo v5 github\\nyolo v5 vs v4\\nyolo v4 github\\nyolo v4 demo\\nyolo v5 paper\\nyolo v5 tutorial\\nyolo v4 pytorch\\nyolo v4 video\\nyolo v5 tutorial\\nyolo v5 object\\ndetection\\nyolo v4 tiny\\nyolo v4 colab\\nyolo v5 vs v4\\nyolo v5 colab\\nyolo v4 vs v5\\nyolo v4 tiny\\nyolo v5\\nyolo v5 demo\\narchitecture\\nyolo v4\\nyolo v4 google\\nyolo v5 darknet\\nyolo v5 video\\ntensorflow\\ncolab\\nyolo v4 tutorial\\nyolo v4 object\\nyolo v5\\nyolo v5 pytorch\\ndetection\\ntensorflow\\nyolo v4 python\\nyolo v4 training\\nyolo v5\\nyolo v5 paper\\ntensorflow\\ngithub\\nyolo v4 training\\nyolo v4\\nyolo v5 tensorrt\\nyolo v5\\n6(2).\\ntensorflow\\narchitecture\\n', 'type': 'composite'}\n",
      "----\n",
      "ID: chunk-1\n",
      "Content: A Review of Yolo Algorithm Developments\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Abstract\n",
      "Abstract\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "1. Introduction\n",
      "1. Introduction\n",
      "You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\n",
      "You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\n",
      "This paper will compare the main differences among the five YOLO versions from both conceptual designs\n",
      "This paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\n",
      "\n",
      "Score: 0.56603241\n",
      "Metadata: {'chunk_id': 1.0, 'content': 'A Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n1. Introduction\\n1. Introduction\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\\n', 'text': 'A Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n1. Introduction\\n1. Introduction\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\\n', 'type': 'composite'}\n",
      "----\n",
      "ID: chunk-3\n",
      "Content: www.elsevier.com/locate/procedia\n",
      "Procedia Computer Science 00 (2018) 000–000\n",
      "The core of the YOLO target detection algorithm lies in the model's small size and fast calculation speed. The structure of YOLO is straightforward. It can directly output the position and category of the bounding box through the neural network. The speed of YOLO is fast because YOLO only needs to put the picture into the network to get the final detection result, so YOLO can also realize the time detection of video. YOLO directly uses the global image for detection, which can encode the global information and reduce the error of detecting the background as the object. YOLO has a strong generalization ability because YOLO can learn highly generalized features to be transferred to other fields. It converts the problem of target detection into a regression problem, but detection accuracy needs to be improved. YOLO's test results are poor for objects that are very close to each other and in groups. This poor performance is because only two boxes in the grid are predicted and only belong to a new class of objects of the same category, so an abnormal aspect ratio appears, and other conditions, such as weak generalization ability.\n",
      "www.elsevier.com/locate/procedia\n",
      "The 8th International Conference on Information Technology and Quantitative Management\n",
      "The 8th International Conference on Information Technology and Quantitative Management\n",
      "(ITQM 2020 & 2021)\n",
      "(ITQM 2020 & 2021)\n",
      "A Review of Yolo Algorithm Developments\n",
      "A Review of Yolo Algorithm Developments\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University),\n",
      "Due to the loss function, the positioning error is the main reason for improving the detection efficiency. Especially the handling of large and small objects needs to be strengthened. In the implementation, the most important thing is how to design the loss function so that these three aspects can be well balanced. YOLO uses multiple lower sampling layers, and the target features learned from the network are not exhaustive so that the detection effect will be improved.\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University),\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Abstract\n",
      "Abstract\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\n",
      "of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\n",
      "The original YOLO architecture consists of 24 convolution layers, followed by two fully connected layers. YOLO predict multiple bounding boxes per grid cell but those bounding boxes having highest Intersection Over Union (IOU) with the ground truth is selected, which is known as non-maxima suppression [13].\n",
      "of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\n",
      "remarks and insightful results. The results show the differences and similarities among the YOLO versions and between\n",
      "remarks and insightful results. The results show the differences and similarities among the YOLO versions and between\n",
      "YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\n",
      "YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\n",
      "ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\n",
      "YOLO has two defects: one is inaccurate positioning, and the other is the lower recall rate compared with the method based on area recommendations. Therefore, YOLO V2 mainly improves in these two aspects. Besides, YOLO V2 does not deepen or broaden the network but simplifies the network.\n",
      "\n",
      "Score: 0.564149857\n",
      "Metadata: {'chunk_id': 3.0, 'content': \"www.elsevier.com/locate/procedia\\nProcedia Computer Science 00 (2018) 000–000\\nThe core of the YOLO target detection algorithm lies in the model's small size and fast calculation speed. The structure of YOLO is straightforward. It can directly output the position and category of the bounding box through the neural network. The speed of YOLO is fast because YOLO only needs to put the picture into the network to get the final detection result, so YOLO can also realize the time detection of video. YOLO directly uses the global image for detection, which can encode the global information and reduce the error of detecting the background as the object. YOLO has a strong generalization ability because YOLO can learn highly generalized features to be transferred to other fields. It converts the problem of target detection into a regression problem, but detection accuracy needs to be improved. YOLO's test results are poor for objects that are very close to each other and in groups. This poor performance is because only two boxes in the grid are predicted and only belong to a new class of objects of the same category, so an abnormal aspect ratio appears, and other conditions, such as weak generalization ability.\\nwww.elsevier.com/locate/procedia\\nThe 8th International Conference on Information Technology and Quantitative Management\\nThe 8th International Conference on Information Technology and Quantitative Management\\n(ITQM 2020 & 2021)\\n(ITQM 2020 & 2021)\\nA Review of Yolo Algorithm Developments\\nA Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University),\\nDue to the loss function, the positioning error is the main reason for improving the detection efficiency. Especially the handling of large and small objects needs to be strengthened. In the implementation, the most important thing is how to design the loss function so that these three aspects can be well balanced. YOLO uses multiple lower sampling layers, and the target features learned from the network are not exhaustive so that the detection effect will be improved.\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University),\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\\nof the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\\nThe original YOLO architecture consists of 24 convolution layers, followed by two fully connected layers. YOLO predict multiple bounding boxes per grid cell but those bounding boxes having highest Intersection Over Union (IOU) with the ground truth is selected, which is known as non-maxima suppression [13].\\nof the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\\nremarks and insightful results. The results show the differences and similarities among the YOLO versions and between\\nremarks and insightful results. The results show the differences and similarities among the YOLO versions and between\\nYOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\\nYOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\\nongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\\nYOLO has two defects: one is inaccurate positioning, and the other is the lower recall rate compared with the method based on area recommendations. Therefore, YOLO V2 mainly improves in these two aspects. Besides, YOLO V2 does not deepen or broaden the network but simplifies the network.\\n\", 'text': \"www.elsevier.com/locate/procedia\\nProcedia Computer Science 00 (2018) 000–000\\nThe core of the YOLO target detection algorithm lies in the model's small size and fast calculation speed. The structure of YOLO is straightforward. It can directly output the position and category of the bounding box through the neural network. The speed of YOLO is fast because YOLO only needs to put the picture into the network to get the final detection result, so YOLO can also realize the time detection of video. YOLO directly uses the global image for detection, which can encode the global information and reduce the error of detecting the background as the object. YOLO has a strong generalization ability because YOLO can learn highly generalized features to be transferred to other fields. It converts the problem of target detection into a regression problem, but detection accuracy needs to be improved. YOLO's test results are poor for objects that are very close to each other and in groups. This poor performance is because only two boxes in the grid are predicted and only belong to a new class of objects of the same category, so an abnormal aspect ratio appears, and other conditions, such as weak generalization ability.\\nwww.elsevier.com/locate/procedia\\nThe 8th International Conference on Information Technology and Quantitative Management\\nThe 8th International Conference on Information Technology and Quantitative Management\\n(ITQM 2020 & 2021)\\n(ITQM 2020 & 2021)\\nA Review of Yolo Algorithm Developments\\nA Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University),\\nDue to the loss function, the positioning error is the main reason for improving the detection efficiency. Especially the handling of large and small objects needs to be strengthened. In the implementation, the most important thing is how to design the loss function so that these three aspects can be well balanced. YOLO uses multiple lower sampling layers, and the target features learned from the network are not exhaustive so that the detection effect will be improved.\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University),\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\\nof the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\\nThe original YOLO architecture consists of 24 convolution layers, followed by two fully connected layers. YOLO predict multiple bounding boxes per grid cell but those bounding boxes having highest Intersection Over Union (IOU) with the ground truth is selected, which is known as non-maxima suppression [13].\\nof the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\\nremarks and insightful results. The results show the differences and similarities among the YOLO versions and between\\nremarks and insightful results. The results show the differences and similarities among the YOLO versions and between\\nYOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\\nYOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\\nongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\\nYOLO has two defects: one is inaccurate positioning, and the other is the lower recall rate compared with the method based on area recommendations. Therefore, YOLO V2 mainly improves in these two aspects. Besides, YOLO V2 does not deepen or broaden the network but simplifies the network.\\n\", 'type': 'composite'}\n",
      "----\n",
      "ID: chunk-4\n",
      "Content: ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\n",
      "recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\n",
      "recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "Two improvements of YOLO V2: Better and Faster.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\n",
      "Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\n",
      "2.2 Better\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "\n",
      "Score: 0.560558319\n",
      "Metadata: {'chunk_id': 4.0, 'content': 'ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\\nrecognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\\nrecognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nTwo improvements of YOLO V2: Better and Faster.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nSelection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\\nSelection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\\n2.2 Better\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n', 'text': 'ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\\nrecognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\\nrecognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nTwo improvements of YOLO V2: Better and Faster.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nSelection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\\nSelection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\\n2.2 Better\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n', 'type': 'composite'}\n",
      "----\n",
      "ID: chunk-10\n",
      "Content: This section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO algorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior work on object detection repurposes classififiers to perform detection [14]. YOLO's performance was competitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second one gives more details about people's insights. Both subsections use numerical data and text show the YOLO is still ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we have deleted the YOLO V1 results in this section due to the noise).\n",
      "3.1 Trends\n",
      "This subsection has collected the publication data for displaying the trends. Table 1 gives us the academic research paper numbers of each version. The breakdown illustrates the research paper number has increasing a lot in the year 2019 and year 2020. Besides, YOLO V3 and V2 versions have attracted most of the researcher's eyes, although the time fact can be another element. V4 and V5 versions' number is less because they are very new.\n",
      "Table 1. YOLO versions breakdown by years\n",
      "<table><thead><tr><th></th><th>YOLO V2</th><th>YOLO V3</th><th>YOLO V4</th><th>YOLO V5</th><th>Total</th></tr></thead><tbody><tr><td>2016</td><td>ie)</td><td>ie)</td><td>ie)</td><td>ie)</td><td>0</td></tr><tr><td>2017</td><td>5</td><td>ie)</td><td>ie)</td><td>ie)</td><td>5</td></tr><tr><td>2018</td><td>47</td><td>19</td><td>ie)</td><td>ie)</td><td>66</td></tr><tr><td>2019</td><td>48</td><td>210</td><td>ie)</td><td>ie)</td><td>258</td></tr><tr><td>2020</td><td>36</td><td>496</td><td>81</td><td>13</td><td>626</td></tr><tr><td>Total</td><td>136</td><td>725</td><td>81</td><td>13</td><td>955</td></tr></tbody></table>\n",
      "Fig. 1 presents the interests over time. The data is based on the web search performance, including news search, image search, and YouTube search. The scale is a relative measurement. The highest point is 100, and the lowest is zero. For example, a value of 50 means that the term is half as popular. Statistically, the value of zero means there data maybe not enough or people are not interested in this topic.\n",
      "The figure shows that the V2 and V3 versions are more prevalent most of the time. However, after April 2020, V4 and V5 are getting more popular. This result matches the numerical results from previous Table 1.\n",
      "Jiang et al./ Procedia Computer Science 00 (2017) 000–000\n",
      "Author name / Procedia Computer Science 00 (2017) 000–000\n",
      "2.4 Relationship\n",
      "Because YOLO and YOLO V2 are not effective in detecting small targets, multi-scale detection is added\n",
      "to YOLO V3. YOLO V3 is a well-received master of the previous generations. YOLO V4 sorted out and tried\n",
      "all possible optimizations in the entire process and found the best effect in each permutation and combination.\n",
      "YOLOv4 runs twice faster than EfficientDet with comparable performance. Improves YOLOv3’s AP and FPS\n",
      "by 10% and 12%, respectively [15]. YOLO V5 can flexibly control models from 10+M to 200+M, and its small\n",
      "model is very impressive. The overall network diagrams of YOLO V3 to YOLO V5 are similar, but they also\n",
      "focus on detecting objects of different sizes from three different scales.\n",
      "3. Public Data Insights\n",
      "This section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO\n",
      "algorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior\n",
      "work on object detection repurposes classififiers to perform detection [14]. YOLO's performance was\n",
      "competitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second\n",
      "one gives more details about people's insights. Both subsections use numerical data and text show the YOLO is\n",
      "still ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we\n",
      "have deleted the YOLO V1 results in this section due to the noise).\n",
      "3.2 Public queries\n",
      "3.1 Trends\n",
      "This subsection gives us more details about the public's interest. We use Google and YouTube as the baseline platforms to compare the four YOLO versions. According to the search keyword analysis. Table 2 and Table 3 summarized the top ten queries for each of them.\n",
      "\n",
      "Score: 0.543324471\n",
      "Metadata: {'chunk_id': 10.0, 'content': \"This section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO algorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior work on object detection repurposes classififiers to perform detection [14]. YOLO's performance was competitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second one gives more details about people's insights. Both subsections use numerical data and text show the YOLO is still ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we have deleted the YOLO V1 results in this section due to the noise).\\n3.1 Trends\\nThis subsection has collected the publication data for displaying the trends. Table 1 gives us the academic research paper numbers of each version. The breakdown illustrates the research paper number has increasing a lot in the year 2019 and year 2020. Besides, YOLO V3 and V2 versions have attracted most of the researcher's eyes, although the time fact can be another element. V4 and V5 versions' number is less because they are very new.\\nTable 1. YOLO versions breakdown by years\\n<table><thead><tr><th></th><th>YOLO V2</th><th>YOLO V3</th><th>YOLO V4</th><th>YOLO V5</th><th>Total</th></tr></thead><tbody><tr><td>2016</td><td>ie)</td><td>ie)</td><td>ie)</td><td>ie)</td><td>0</td></tr><tr><td>2017</td><td>5</td><td>ie)</td><td>ie)</td><td>ie)</td><td>5</td></tr><tr><td>2018</td><td>47</td><td>19</td><td>ie)</td><td>ie)</td><td>66</td></tr><tr><td>2019</td><td>48</td><td>210</td><td>ie)</td><td>ie)</td><td>258</td></tr><tr><td>2020</td><td>36</td><td>496</td><td>81</td><td>13</td><td>626</td></tr><tr><td>Total</td><td>136</td><td>725</td><td>81</td><td>13</td><td>955</td></tr></tbody></table>\\nFig. 1 presents the interests over time. The data is based on the web search performance, including news search, image search, and YouTube search. The scale is a relative measurement. The highest point is 100, and the lowest is zero. For example, a value of 50 means that the term is half as popular. Statistically, the value of zero means there data maybe not enough or people are not interested in this topic.\\nThe figure shows that the V2 and V3 versions are more prevalent most of the time. However, after April 2020, V4 and V5 are getting more popular. This result matches the numerical results from previous Table 1.\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\n2.4 Relationship\\nBecause YOLO and YOLO V2 are not effective in detecting small targets, multi-scale detection is added\\nto YOLO V3. YOLO V3 is a well-received master of the previous generations. YOLO V4 sorted out and tried\\nall possible optimizations in the entire process and found the best effect in each permutation and combination.\\nYOLOv4 runs twice faster than EfficientDet with comparable performance. Improves YOLOv3’s AP and FPS\\nby 10% and 12%, respectively [15]. YOLO V5 can flexibly control models from 10+M to 200+M, and its small\\nmodel is very impressive. The overall network diagrams of YOLO V3 to YOLO V5 are similar, but they also\\nfocus on detecting objects of different sizes from three different scales.\\n3. Public Data Insights\\nThis section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO\\nalgorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior\\nwork on object detection repurposes classififiers to perform detection [14]. YOLO's performance was\\ncompetitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second\\none gives more details about people's insights. Both subsections use numerical data and text show the YOLO is\\nstill ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we\\nhave deleted the YOLO V1 results in this section due to the noise).\\n3.2 Public queries\\n3.1 Trends\\nThis subsection gives us more details about the public's interest. We use Google and YouTube as the baseline platforms to compare the four YOLO versions. According to the search keyword analysis. Table 2 and Table 3 summarized the top ten queries for each of them.\\n\", 'text': \"This section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO algorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior work on object detection repurposes classififiers to perform detection [14]. YOLO's performance was competitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second one gives more details about people's insights. Both subsections use numerical data and text show the YOLO is still ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we have deleted the YOLO V1 results in this section due to the noise).\\n3.1 Trends\\nThis subsection has collected the publication data for displaying the trends. Table 1 gives us the academic research paper numbers of each version. The breakdown illustrates the research paper number has increasing a lot in the year 2019 and year 2020. Besides, YOLO V3 and V2 versions have attracted most of the researcher's eyes, although the time fact can be another element. V4 and V5 versions' number is less because they are very new.\\nTable 1. YOLO versions breakdown by years\\n<table><thead><tr><th></th><th>YOLO V2</th><th>YOLO V3</th><th>YOLO V4</th><th>YOLO V5</th><th>Total</th></tr></thead><tbody><tr><td>2016</td><td>ie)</td><td>ie)</td><td>ie)</td><td>ie)</td><td>0</td></tr><tr><td>2017</td><td>5</td><td>ie)</td><td>ie)</td><td>ie)</td><td>5</td></tr><tr><td>2018</td><td>47</td><td>19</td><td>ie)</td><td>ie)</td><td>66</td></tr><tr><td>2019</td><td>48</td><td>210</td><td>ie)</td><td>ie)</td><td>258</td></tr><tr><td>2020</td><td>36</td><td>496</td><td>81</td><td>13</td><td>626</td></tr><tr><td>Total</td><td>136</td><td>725</td><td>81</td><td>13</td><td>955</td></tr></tbody></table>\\nFig. 1 presents the interests over time. The data is based on the web search performance, including news search, image search, and YouTube search. The scale is a relative measurement. The highest point is 100, and the lowest is zero. For example, a value of 50 means that the term is half as popular. Statistically, the value of zero means there data maybe not enough or people are not interested in this topic.\\nThe figure shows that the V2 and V3 versions are more prevalent most of the time. However, after April 2020, V4 and V5 are getting more popular. This result matches the numerical results from previous Table 1.\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\n2.4 Relationship\\nBecause YOLO and YOLO V2 are not effective in detecting small targets, multi-scale detection is added\\nto YOLO V3. YOLO V3 is a well-received master of the previous generations. YOLO V4 sorted out and tried\\nall possible optimizations in the entire process and found the best effect in each permutation and combination.\\nYOLOv4 runs twice faster than EfficientDet with comparable performance. Improves YOLOv3’s AP and FPS\\nby 10% and 12%, respectively [15]. YOLO V5 can flexibly control models from 10+M to 200+M, and its small\\nmodel is very impressive. The overall network diagrams of YOLO V3 to YOLO V5 are similar, but they also\\nfocus on detecting objects of different sizes from three different scales.\\n3. Public Data Insights\\nThis section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO\\nalgorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior\\nwork on object detection repurposes classififiers to perform detection [14]. YOLO's performance was\\ncompetitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second\\none gives more details about people's insights. Both subsections use numerical data and text show the YOLO is\\nstill ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we\\nhave deleted the YOLO V1 results in this section due to the noise).\\n3.2 Public queries\\n3.1 Trends\\nThis subsection gives us more details about the public's interest. We use Google and YouTube as the baseline platforms to compare the four YOLO versions. According to the search keyword analysis. Table 2 and Table 3 summarized the top ten queries for each of them.\\n\", 'type': 'composite'}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query_text = \"what is YOLO\"\n",
    "query_vector = embeddings.embed_query(query_text)\n",
    "\n",
    "# Step 2: Query Pinecone index\n",
    "results = index.query(\n",
    "    vector=query_vector,\n",
    "    top_k=5,  # number of most similar results\n",
    "    include_metadata=True,\n",
    "    include_values=True\n",
    ")\n",
    "\n",
    "# Step 3: Print results\n",
    "for match in results['matches']:\n",
    "    print(f\"ID: {match['id']}\")\n",
    "    print(f\"Content: {match['metadata'].get('content', '')}\")\n",
    "    print(f\"Score: {match['score']}\")\n",
    "    print(f\"Metadata: {match['metadata']}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "import pinecone\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\n",
      "There is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\n",
      "This research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\n",
      "Author name / Procedia Computer Science 00 (2017) 000–000\n",
      "Jiang et al./ Procedia Computer Science 00 (2017) 000–000\n",
      "yolo v2 algorithm\n",
      "yolo v2\n",
      "yolo v3 object\n",
      "yolo v3 code\n",
      "References\n",
      "tensorflow\n",
      "detection\n",
      "Table 3. Top ten queries for each YOLO version (V4 and V5)\n",
      "YOLO V4\n",
      "YOLO V5\n",
      "GOOGLE\n",
      "YOUTUBE\n",
      "GOOGLE\n",
      "YOUTUBE\n",
      "yolo v4\n",
      "yolo v4\n",
      "yolo v5\n",
      "yolo v5\n",
      "yolo v4 alexeyab\n",
      "yolo v4 tutorial\n",
      "yolo v5 github\n",
      "yolo v5 vs v4\n",
      "yolo v4 github\n",
      "yolo v4 demo\n",
      "yolo v5 paper\n",
      "yolo v5 tutorial\n",
      "yolo v4 pytorch\n",
      "yolo v4 video\n",
      "yolo v5 tutorial\n",
      "yolo v5 object\n",
      "detection\n",
      "yolo v4 tiny\n",
      "yolo v4 colab\n",
      "yolo v5 vs v4\n",
      "yolo v5 colab\n",
      "yolo v4 vs v5\n",
      "yolo v4 tiny\n",
      "yolo v5\n",
      "yolo v5 demo\n",
      "architecture\n",
      "yolo v4\n",
      "yolo v4 google\n",
      "yolo v5 darknet\n",
      "yolo v5 video\n",
      "tensorflow\n",
      "colab\n",
      "yolo v4 tutorial\n",
      "yolo v4 object\n",
      "yolo v5\n",
      "yolo v5 pytorch\n",
      "detection\n",
      "tensorflow\n",
      "yolo v4 python\n",
      "yolo v4 training\n",
      "yolo v5\n",
      "yolo v5 paper\n",
      "tensorflow\n",
      "github\n",
      "yolo v4 training\n",
      "yolo v4\n",
      "yolo v5 tensorrt\n",
      "yolo v5\n",
      "6(2).\n",
      "tensorflow\n",
      "architecture\n",
      "\n",
      "A Review of Yolo Algorithm Developments\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Abstract\n",
      "Abstract\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "1. Introduction\n",
      "1. Introduction\n",
      "You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\n",
      "You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\n",
      "This paper will compare the main differences among the five YOLO versions from both conceptual designs\n",
      "This paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\n",
      "\n",
      "www.elsevier.com/locate/procedia\n",
      "Procedia Computer Science 00 (2018) 000–000\n",
      "The core of the YOLO target detection algorithm lies in the model's small size and fast calculation speed. The structure of YOLO is straightforward. It can directly output the position and category of the bounding box through the neural network. The speed of YOLO is fast because YOLO only needs to put the picture into the network to get the final detection result, so YOLO can also realize the time detection of video. YOLO directly uses the global image for detection, which can encode the global information and reduce the error of detecting the background as the object. YOLO has a strong generalization ability because YOLO can learn highly generalized features to be transferred to other fields. It converts the problem of target detection into a regression problem, but detection accuracy needs to be improved. YOLO's test results are poor for objects that are very close to each other and in groups. This poor performance is because only two boxes in the grid are predicted and only belong to a new class of objects of the same category, so an abnormal aspect ratio appears, and other conditions, such as weak generalization ability.\n",
      "www.elsevier.com/locate/procedia\n",
      "The 8th International Conference on Information Technology and Quantitative Management\n",
      "The 8th International Conference on Information Technology and Quantitative Management\n",
      "(ITQM 2020 & 2021)\n",
      "(ITQM 2020 & 2021)\n",
      "A Review of Yolo Algorithm Developments\n",
      "A Review of Yolo Algorithm Developments\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University),\n",
      "Due to the loss function, the positioning error is the main reason for improving the detection efficiency. Especially the handling of large and small objects needs to be strengthened. In the implementation, the most important thing is how to design the loss function so that these three aspects can be well balanced. YOLO uses multiple lower sampling layers, and the target features learned from the network are not exhaustive so that the detection effect will be improved.\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University),\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Abstract\n",
      "Abstract\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\n",
      "of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\n",
      "The original YOLO architecture consists of 24 convolution layers, followed by two fully connected layers. YOLO predict multiple bounding boxes per grid cell but those bounding boxes having highest Intersection Over Union (IOU) with the ground truth is selected, which is known as non-maxima suppression [13].\n",
      "of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\n",
      "remarks and insightful results. The results show the differences and similarities among the YOLO versions and between\n",
      "remarks and insightful results. The results show the differences and similarities among the YOLO versions and between\n",
      "YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\n",
      "YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\n",
      "ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\n",
      "YOLO has two defects: one is inaccurate positioning, and the other is the lower recall rate compared with the method based on area recommendations. Therefore, YOLO V2 mainly improves in these two aspects. Besides, YOLO V2 does not deepen or broaden the network but simplifies the network.\n",
      "\n",
      "ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\n",
      "recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\n",
      "recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "Two improvements of YOLO V2: Better and Faster.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\n",
      "Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\n",
      "2.2 Better\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "\n",
      "This section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO algorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior work on object detection repurposes classififiers to perform detection [14]. YOLO's performance was competitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second one gives more details about people's insights. Both subsections use numerical data and text show the YOLO is still ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we have deleted the YOLO V1 results in this section due to the noise).\n",
      "3.1 Trends\n",
      "This subsection has collected the publication data for displaying the trends. Table 1 gives us the academic research paper numbers of each version. The breakdown illustrates the research paper number has increasing a lot in the year 2019 and year 2020. Besides, YOLO V3 and V2 versions have attracted most of the researcher's eyes, although the time fact can be another element. V4 and V5 versions' number is less because they are very new.\n",
      "Table 1. YOLO versions breakdown by years\n",
      "<table><thead><tr><th></th><th>YOLO V2</th><th>YOLO V3</th><th>YOLO V4</th><th>YOLO V5</th><th>Total</th></tr></thead><tbody><tr><td>2016</td><td>ie)</td><td>ie)</td><td>ie)</td><td>ie)</td><td>0</td></tr><tr><td>2017</td><td>5</td><td>ie)</td><td>ie)</td><td>ie)</td><td>5</td></tr><tr><td>2018</td><td>47</td><td>19</td><td>ie)</td><td>ie)</td><td>66</td></tr><tr><td>2019</td><td>48</td><td>210</td><td>ie)</td><td>ie)</td><td>258</td></tr><tr><td>2020</td><td>36</td><td>496</td><td>81</td><td>13</td><td>626</td></tr><tr><td>Total</td><td>136</td><td>725</td><td>81</td><td>13</td><td>955</td></tr></tbody></table>\n",
      "Fig. 1 presents the interests over time. The data is based on the web search performance, including news search, image search, and YouTube search. The scale is a relative measurement. The highest point is 100, and the lowest is zero. For example, a value of 50 means that the term is half as popular. Statistically, the value of zero means there data maybe not enough or people are not interested in this topic.\n",
      "The figure shows that the V2 and V3 versions are more prevalent most of the time. However, after April 2020, V4 and V5 are getting more popular. This result matches the numerical results from previous Table 1.\n",
      "Jiang et al./ Procedia Computer Science 00 (2017) 000–000\n",
      "Author name / Procedia Computer Science 00 (2017) 000–000\n",
      "2.4 Relationship\n",
      "Because YOLO and YOLO V2 are not effective in detecting small targets, multi-scale detection is added\n",
      "to YOLO V3. YOLO V3 is a well-received master of the previous generations. YOLO V4 sorted out and tried\n",
      "all possible optimizations in the entire process and found the best effect in each permutation and combination.\n",
      "YOLOv4 runs twice faster than EfficientDet with comparable performance. Improves YOLOv3’s AP and FPS\n",
      "by 10% and 12%, respectively [15]. YOLO V5 can flexibly control models from 10+M to 200+M, and its small\n",
      "model is very impressive. The overall network diagrams of YOLO V3 to YOLO V5 are similar, but they also\n",
      "focus on detecting objects of different sizes from three different scales.\n",
      "3. Public Data Insights\n",
      "This section gives a brief overview of the YOLO versions through the public data. In 2015, the YOLO\n",
      "algorithm was published, which is used for object detection. YOLO, a new approach to object detection. Prior\n",
      "work on object detection repurposes classififiers to perform detection [14]. YOLO's performance was\n",
      "competitive, but it still has room for improvement. The first subsection presents the algorithm trends. The second\n",
      "one gives more details about people's insights. Both subsections use numerical data and text show the YOLO is\n",
      "still ongoing and keeps updating. All the data is collected from GOOGLE (www.google.com) open dataset (we\n",
      "have deleted the YOLO V1 results in this section due to the noise).\n",
      "3.2 Public queries\n",
      "3.1 Trends\n",
      "This subsection gives us more details about the public's interest. We use Google and YouTube as the baseline platforms to compare the four YOLO versions. According to the search keyword analysis. Table 2 and Table 3 summarized the top ten queries for each of them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is YOLO\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {question}\\n')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO (You Only Look Once) is an algorithm used for object detection in the field of artificial intelligence. It is designed to directly output the position and category of bounding boxes through a neural network, allowing for fast detection of objects in images or videos.\n"
     ]
    }
   ],
   "source": [
    "## LLM\n",
    "from langchain.chat_models.base import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "#llm=init_chat_model(\"groq:\")\n",
    "llm\n",
    "### LCEL Chain With retrieval\n",
    "\n",
    "rag_chain=(\n",
    "    RunnableMap(\n",
    "        {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],  \n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- 8. Run Query ---\n",
    "query = {\"question\": \"What is Yolo used for? what is yolo stand for\"}\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
