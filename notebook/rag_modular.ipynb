{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, file_path: str, chunking_strategy: Literal[\"by_title\", \"basic\"], maximum_chunk_characters: int,  combine_text_under_n_chars: int, new_after_n_chars: int):\n",
    "        self.file_path = file_path\n",
    "        self.chunking_strategy = chunking_strategy\n",
    "        self.maximum_chunk_characters = maximum_chunk_characters\n",
    "        self.combine_text_under_n_chars = combine_text_under_n_chars\n",
    "        self.new_after_n_chars = new_after_n_chars\n",
    "\n",
    "    def unstructured_chunks(self):\n",
    "        return partition_pdf(\n",
    "            filename=self.file_path,\n",
    "            infer_table_structure=True,\n",
    "            strategy=\"hi_res\",\n",
    "            chunking_strategy=self.chunking_strategy,\n",
    "            max_characters=self.maximum_chunk_characters,              \n",
    "            combine_text_under_n_chars=self.combine_text_under_n_chars,\n",
    "            new_after_n_chars=self.new_after_n_chars,\n",
    "\n",
    "        )\n",
    "\n",
    "    def split_elements_from_chunks(self, chunk):\n",
    "        \"\"\"\n",
    "        Extract text/table content from a CompositeElement chunk \n",
    "        and combine into one string.\n",
    "        \"\"\"\n",
    "        if \"CompositeElement\" not in str(type(chunk)):\n",
    "            return \"\"\n",
    "\n",
    "        chunk_elements = chunk.metadata.orig_elements\n",
    "        combined_content = \"\"\n",
    "\n",
    "        for element in chunk_elements:\n",
    "            if \"Text\" in str(type(element)):\n",
    "                combined_content += element.text + \"\\n\"\n",
    "            elif \"Table\" in str(type(element)):\n",
    "                combined_content += element.metadata.text_as_html + \"\\n\"\n",
    "            # elif \"Image\" in str(type(element)):\n",
    "            #     combined_content += \"[IMAGE]\\n\"\n",
    "\n",
    "        return combined_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.vectorstores import Pinecone as LC_Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, pc_api_key:str, openai_api_key:str, index_name:str, dimension:int, model:str, metric:int, region=\"us-east-1\"):\n",
    "        self.pc_api_key = pc_api_key\n",
    "        self.openai_api_key = openai_api_key\n",
    "        #self.pc = Pinecone(api_key=self.pc_api_key)\n",
    "        self.index_name = index_name\n",
    "        #self.embeddings = OpenAIEmbeddings(model=self.model, api_key=self.openai_api_key)\n",
    "        self.model = model\n",
    "        self.dimension = dimension \n",
    "        self.metric = metric\n",
    "        self.region = region\n",
    "\n",
    "    def create_vector_db(self):\n",
    "        print(self.pc_api_key)\n",
    "        pc = Pinecone(api_key=self.pc_api_key)\n",
    "        if self.index_name not in pc.list_indexes().names():\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=self.dimension,\n",
    "                metric=self.metric,\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=self.region)\n",
    "            )\n",
    "            print(f\"Index '{self.index_name}' created!\")\n",
    "\n",
    "        else:\n",
    "            # Connect to existing index\n",
    "            index = pc.Index(self.index_name)\n",
    "            print(f\"Index '{self.index_name}' already exists and is ready.\")\n",
    "\n",
    "            \n",
    "    def upsert_chunk(self, chunk_id: str, content: str):\n",
    "        embeddings = OpenAIEmbeddings(model=self.model, api_key=self.openai_api_key)\n",
    "        \"\"\"\n",
    "        Embed the given chunk content and upsert into Pinecone.\n",
    "        \"\"\"\n",
    "        embedded_chunk = embeddings.embed_documents([content])[0]\n",
    "        pc = Pinecone(api_key=self.pc_api_key)\n",
    "        index = pc.Index(self.index_name)\n",
    "        index.upsert([{\n",
    "            \"id\": chunk_id,\n",
    "            \"values\": embedded_chunk,\n",
    "            \"metadata\": {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"type\": \"composite\",\n",
    "                \"content\": content,\n",
    "                \"text\": content\n",
    "            }\n",
    "        }])\n",
    "\n",
    "    def query_pinecone(self, query_text, top_k:int):\n",
    "        \"\"\" \n",
    "        Query Pinecone index with a text query and return results.\n",
    "\n",
    "        Args:\n",
    "        index: Pinecone index object\n",
    "        embeddings: Embedding model object\n",
    "        query_text (str): Input query text\n",
    "        top_k (int): Number of top results to fetch\n",
    "\n",
    "        Returns:\n",
    "            list: List of result dictionaries with id, content, score, and metadata\n",
    "        \"\"\"\n",
    "    # Step 1: Convert query text to vector\n",
    "        embeddings = OpenAIEmbeddings(model=self.model, api_key=self.openai_api_key)\n",
    "        query_vector = embeddings.embed_query(query_text)\n",
    "        pc = Pinecone(api_key=self.pc_api_key)\n",
    "        index = pc.Index(self.index_name)\n",
    "        # Step 2: Query Pinecone index\n",
    "        results = index.query(\n",
    "            vector=query_vector,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            include_values=True\n",
    "        )\n",
    "\n",
    "        # Step 3: Format results\n",
    "        formatted_results = []\n",
    "        for match in results['matches']:\n",
    "            formatted_results.append({\n",
    "                \"id\": match['id'],\n",
    "                \"content\": match['metadata'].get('content', ''),\n",
    "                \"score\": match['score'],\n",
    "                \"metadata\": match['metadata']\n",
    "            })\n",
    "\n",
    "        return formatted_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PDFProcessor(file_path = \"/Users/rohan/Documents/RAG Pipeline/RAG-Pipeline/artifacts/research_paper_yolo.pdf\", \n",
    "                         chunking_strategy = \"by_title\", \n",
    "                         maximum_chunk_characters = 5000,  \n",
    "                         combine_text_under_n_chars = 2000, \n",
    "                         new_after_n_chars = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "chunks = processor.unstructured_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = VectorStore(pc_api_key=PINECONE_API_KEY, \n",
    "                 openai_api_key=OPENAI_API_KEY, \n",
    "                 index_name='newindex', \n",
    "                 #embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key=OPENAI_API_KEY\n",
    "                 dimension=1536, \n",
    "                 model=\"text-embedding-3-small\", \n",
    "                 metric=\"cosine\", \n",
    "                 region=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_2VkJxA_EjAnGwoD7aDLXUxksRjfxCw5MegC4TLBd294aN7dZpdYJRn3u2sTk1Zx7TqFNRQ\n",
      "Index 'newindex' already exists and is ready.\n"
     ]
    }
   ],
   "source": [
    "pc.create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "chunks = processor.unstructured_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    combined_content = processor.split_elements_from_chunks(chunk)\n",
    "    if combined_content:\n",
    "        chunk_id = f\"chunk-{i}\"\n",
    "        pc.upsert_chunk(chunk_id, combined_content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'chunk-12',\n",
       "  'content': 'This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\\nThere is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\\nThis research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nyolo v2 algorithm\\nyolo v2\\nyolo v3 object\\nyolo v3 code\\nReferences\\ntensorflow\\ndetection\\nTable 3. Top ten queries for each YOLO version (V4 and V5)\\nYOLO V4\\nYOLO V5\\nGOOGLE\\nYOUTUBE\\nGOOGLE\\nYOUTUBE\\nyolo v4\\nyolo v4\\nyolo v5\\nyolo v5\\nyolo v4 alexeyab\\nyolo v4 tutorial\\nyolo v5 github\\nyolo v5 vs v4\\nyolo v4 github\\nyolo v4 demo\\nyolo v5 paper\\nyolo v5 tutorial\\nyolo v4 pytorch\\nyolo v4 video\\nyolo v5 tutorial\\nyolo v5 object\\ndetection\\nyolo v4 tiny\\nyolo v4 colab\\nyolo v5 vs v4\\nyolo v5 colab\\nyolo v4 vs v5\\nyolo v4 tiny\\nyolo v5\\nyolo v5 demo\\narchitecture\\nyolo v4\\nyolo v4 google\\nyolo v5 darknet\\nyolo v5 video\\ntensorflow\\ncolab\\nyolo v4 tutorial\\nyolo v4 object\\nyolo v5\\nyolo v5 pytorch\\ndetection\\ntensorflow\\nyolo v4 python\\nyolo v4 training\\nyolo v5\\nyolo v5 paper\\ntensorflow\\ngithub\\nyolo v4 training\\nyolo v4\\nyolo v5 tensorrt\\nyolo v5\\n6(2).\\ntensorflow\\narchitecture\\n',\n",
       "  'score': 0.54126364,\n",
       "  'metadata': {'chunk_id': 'chunk-12',\n",
       "   'content': 'This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\\nThere is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\\nThis research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nyolo v2 algorithm\\nyolo v2\\nyolo v3 object\\nyolo v3 code\\nReferences\\ntensorflow\\ndetection\\nTable 3. Top ten queries for each YOLO version (V4 and V5)\\nYOLO V4\\nYOLO V5\\nGOOGLE\\nYOUTUBE\\nGOOGLE\\nYOUTUBE\\nyolo v4\\nyolo v4\\nyolo v5\\nyolo v5\\nyolo v4 alexeyab\\nyolo v4 tutorial\\nyolo v5 github\\nyolo v5 vs v4\\nyolo v4 github\\nyolo v4 demo\\nyolo v5 paper\\nyolo v5 tutorial\\nyolo v4 pytorch\\nyolo v4 video\\nyolo v5 tutorial\\nyolo v5 object\\ndetection\\nyolo v4 tiny\\nyolo v4 colab\\nyolo v5 vs v4\\nyolo v5 colab\\nyolo v4 vs v5\\nyolo v4 tiny\\nyolo v5\\nyolo v5 demo\\narchitecture\\nyolo v4\\nyolo v4 google\\nyolo v5 darknet\\nyolo v5 video\\ntensorflow\\ncolab\\nyolo v4 tutorial\\nyolo v4 object\\nyolo v5\\nyolo v5 pytorch\\ndetection\\ntensorflow\\nyolo v4 python\\nyolo v4 training\\nyolo v5\\nyolo v5 paper\\ntensorflow\\ngithub\\nyolo v4 training\\nyolo v4\\nyolo v5 tensorrt\\nyolo v5\\n6(2).\\ntensorflow\\narchitecture\\n',\n",
       "   'text': 'This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\\nThere is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\\nThis research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\\nAuthor name / Procedia Computer Science 00 (2017) 000–000\\nJiang et al./ Procedia Computer Science 00 (2017) 000–000\\nyolo v2 algorithm\\nyolo v2\\nyolo v3 object\\nyolo v3 code\\nReferences\\ntensorflow\\ndetection\\nTable 3. Top ten queries for each YOLO version (V4 and V5)\\nYOLO V4\\nYOLO V5\\nGOOGLE\\nYOUTUBE\\nGOOGLE\\nYOUTUBE\\nyolo v4\\nyolo v4\\nyolo v5\\nyolo v5\\nyolo v4 alexeyab\\nyolo v4 tutorial\\nyolo v5 github\\nyolo v5 vs v4\\nyolo v4 github\\nyolo v4 demo\\nyolo v5 paper\\nyolo v5 tutorial\\nyolo v4 pytorch\\nyolo v4 video\\nyolo v5 tutorial\\nyolo v5 object\\ndetection\\nyolo v4 tiny\\nyolo v4 colab\\nyolo v5 vs v4\\nyolo v5 colab\\nyolo v4 vs v5\\nyolo v4 tiny\\nyolo v5\\nyolo v5 demo\\narchitecture\\nyolo v4\\nyolo v4 google\\nyolo v5 darknet\\nyolo v5 video\\ntensorflow\\ncolab\\nyolo v4 tutorial\\nyolo v4 object\\nyolo v5\\nyolo v5 pytorch\\ndetection\\ntensorflow\\nyolo v4 python\\nyolo v4 training\\nyolo v5\\nyolo v5 paper\\ntensorflow\\ngithub\\nyolo v4 training\\nyolo v4\\nyolo v5 tensorrt\\nyolo v5\\n6(2).\\ntensorflow\\narchitecture\\n',\n",
       "   'type': 'composite'}},\n",
       " {'id': 'chunk-1',\n",
       "  'content': 'A Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n1. Introduction\\n1. Introduction\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\\n',\n",
       "  'score': 0.518290579,\n",
       "  'metadata': {'chunk_id': 'chunk-1',\n",
       "   'content': 'A Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n1. Introduction\\n1. Introduction\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\\n',\n",
       "   'text': 'A Review of Yolo Algorithm Developments\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nPeiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\\nKey Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nChengdu, 610041, China. * Corresponding author. ergudaji@163.com\\nAbstract\\nAbstract\\nObject detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\nfinancial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\\n© 2021 The Authors. Published by Elsevier B.V.\\n© 2021 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\nKeywords: Review; Yolo; Object Detection; Public Data Analysis\\n1. Introduction\\n1. Introduction\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\\nYou Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs\\nThis paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\\n',\n",
       "   'type': 'composite'}}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.query_pinecone(\"what is Yolo\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, pc_api_key, index_name, model, openai_api_key, search_type: str = \"similarity\", k: int = 5):\n",
    "        self.pc_api_key = pc_api_key\n",
    "        self.index_name=index_name\n",
    "        self.model = model\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.search_type = search_type\n",
    "        self.k = k\n",
    "\n",
    "    def create_retriver(self):\n",
    "        pc = Pinecone(api_key=self.pc_api_key)\n",
    "        index = pc.Index(self.index_name)\n",
    "        embeddings = OpenAIEmbeddings(model=self.model,api_key=self.openai_api_key)\n",
    "        vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=self.search_type,\n",
    "            search_kwargs={\"k\": self.k}\n",
    "        )\n",
    "        return retriever\n",
    "\n",
    "    def query(self, query_text: str, retriever):\n",
    "        pc = Pinecone(api_key=self.pc_api_key)\n",
    "        index = pc.Index(self.index_name)\n",
    "        embeddings = OpenAIEmbeddings(model=self.model,api_key=self.openai_api_key)\n",
    "        vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "        return retriever.get_relevant_documents(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "r= Retriever(pc_api_key = PINECONE_API_KEY, index_name = \"newindex\", model = 'text-embedding-3-small' , openai_api_key = OPENAI_API_KEY, search_type= \"similarity\", k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = r.create_retriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=r.query(\"What is Yolo\",r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper gives us a review of the YOLO versions. Here we draw the following remarks. First, the YOLO version has a lot of differences. However, they still have some features in common. Hence, they are still similar. Second. The YOLO versions are still very new, have a lot of room for future research. Especially for scenario implementations.\n",
      "There is still room for future improvement. This paper can focus more on the implementations comparing, such as scenario analysis. Further, the research for YOLO V1 is very limited in this paper. For example, in the trend subsection, both the figure and tabular have ignored YOLO V1. Future research can do better on this point.\n",
      "This research has been partially supported by grants from the National Natural Science Foundation of China (Nos. 71774134, U1811462). This research is also supported by the Fundamental Research Funds for the Central Universities,Southwest Minzu University(Grant Number 2020NGD04,and 2018NZD02).\n",
      "Author name / Procedia Computer Science 00 (2017) 000–000\n",
      "Jiang et al./ Procedia Computer Science 00 (2017) 000–000\n",
      "yolo v2 algorithm\n",
      "yolo v2\n",
      "yolo v3 object\n",
      "yolo v3 code\n",
      "References\n",
      "tensorflow\n",
      "detection\n",
      "Table 3. Top ten queries for each YOLO version (V4 and V5)\n",
      "YOLO V4\n",
      "YOLO V5\n",
      "GOOGLE\n",
      "YOUTUBE\n",
      "GOOGLE\n",
      "YOUTUBE\n",
      "yolo v4\n",
      "yolo v4\n",
      "yolo v5\n",
      "yolo v5\n",
      "yolo v4 alexeyab\n",
      "yolo v4 tutorial\n",
      "yolo v5 github\n",
      "yolo v5 vs v4\n",
      "yolo v4 github\n",
      "yolo v4 demo\n",
      "yolo v5 paper\n",
      "yolo v5 tutorial\n",
      "yolo v4 pytorch\n",
      "yolo v4 video\n",
      "yolo v5 tutorial\n",
      "yolo v5 object\n",
      "detection\n",
      "yolo v4 tiny\n",
      "yolo v4 colab\n",
      "yolo v5 vs v4\n",
      "yolo v5 colab\n",
      "yolo v4 vs v5\n",
      "yolo v4 tiny\n",
      "yolo v5\n",
      "yolo v5 demo\n",
      "architecture\n",
      "yolo v4\n",
      "yolo v4 google\n",
      "yolo v5 darknet\n",
      "yolo v5 video\n",
      "tensorflow\n",
      "colab\n",
      "yolo v4 tutorial\n",
      "yolo v4 object\n",
      "yolo v5\n",
      "yolo v5 pytorch\n",
      "detection\n",
      "tensorflow\n",
      "yolo v4 python\n",
      "yolo v4 training\n",
      "yolo v5\n",
      "yolo v5 paper\n",
      "tensorflow\n",
      "github\n",
      "yolo v4 training\n",
      "yolo v4\n",
      "yolo v5 tensorrt\n",
      "yolo v5\n",
      "6(2).\n",
      "tensorflow\n",
      "architecture\n",
      "\n",
      "A Review of Yolo Algorithm Developments\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Key Laboratory of Electronic and Information Engineering (Southwest Minzu University), Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Abstract\n",
      "Abstract\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) © 2021 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 Peer-review under responsibility of the scientific committee of the The 8th International Conference on Information Technology Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021 and Quantitative Management (ITQM 2020 & 2021)\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "1. Introduction\n",
      "1. Introduction\n",
      "You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object\n",
      "You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research paper only focused on the five main YOLO versions. paper only focused on the five main YOLO versions.\n",
      "This paper will compare the main differences among the five YOLO versions from both conceptual designs\n",
      "This paper will compare the main differences among the five YOLO versions from both conceptual designs and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, features development, limitations, and even relationships for the versions. This reviewing paper will be features development, limitations, and even relationships for the versions. This reviewing paper will be meaningful and insightful for object detection researchers, especially for beginners. meaningful and insightful for object detection researchers, especially for beginners.\n",
      "\n",
      "www.elsevier.com/locate/procedia\n",
      "Procedia Computer Science 00 (2018) 000–000\n",
      "The core of the YOLO target detection algorithm lies in the model's small size and fast calculation speed. The structure of YOLO is straightforward. It can directly output the position and category of the bounding box through the neural network. The speed of YOLO is fast because YOLO only needs to put the picture into the network to get the final detection result, so YOLO can also realize the time detection of video. YOLO directly uses the global image for detection, which can encode the global information and reduce the error of detecting the background as the object. YOLO has a strong generalization ability because YOLO can learn highly generalized features to be transferred to other fields. It converts the problem of target detection into a regression problem, but detection accuracy needs to be improved. YOLO's test results are poor for objects that are very close to each other and in groups. This poor performance is because only two boxes in the grid are predicted and only belong to a new class of objects of the same category, so an abnormal aspect ratio appears, and other conditions, such as weak generalization ability.\n",
      "www.elsevier.com/locate/procedia\n",
      "The 8th International Conference on Information Technology and Quantitative Management\n",
      "The 8th International Conference on Information Technology and Quantitative Management\n",
      "(ITQM 2020 & 2021)\n",
      "(ITQM 2020 & 2021)\n",
      "A Review of Yolo Algorithm Developments\n",
      "A Review of Yolo Algorithm Developments\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Peiyuan Jiang, Daji Ergu*, Fangyao Liu, Ying Cai, Bo Ma\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University),\n",
      "Due to the loss function, the positioning error is the main reason for improving the detection efficiency. Especially the handling of large and small objects needs to be strengthened. In the implementation, the most important thing is how to design the loss function so that these three aspects can be well balanced. YOLO uses multiple lower sampling layers, and the target features learned from the network are not exhaustive so that the detection effect will be improved.\n",
      "Key Laboratory of Electronic and Information Engineering (Southwest Minzu University),\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Chengdu, 610041, China. * Corresponding author. ergudaji@163.com\n",
      "Abstract\n",
      "Abstract\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\n",
      "Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview\n",
      "of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\n",
      "The original YOLO architecture consists of 24 convolution layers, followed by two fully connected layers. YOLO predict multiple bounding boxes per grid cell but those bounding boxes having highest Intersection Over Union (IOU) with the ground truth is selected, which is known as non-maxima suppression [13].\n",
      "of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many\n",
      "remarks and insightful results. The results show the differences and similarities among the YOLO versions and between\n",
      "remarks and insightful results. The results show the differences and similarities among the YOLO versions and between\n",
      "YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\n",
      "YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still\n",
      "ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\n",
      "YOLO has two defects: one is inaccurate positioning, and the other is the lower recall rate compared with the method based on area recommendations. Therefore, YOLO V2 mainly improves in these two aspects. Besides, YOLO V2 does not deepen or broaden the network but simplifies the network.\n",
      "\n",
      "ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target\n",
      "recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\n",
      "recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.\n",
      "Two improvements of YOLO V2: Better and Faster.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "© 2021 The Authors. Published by Elsevier B.V.\n",
      "Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\n",
      "Selection and/or peer-review under responsibility of the organizers of ITQM 2020&2021\n",
      "2.2 Better\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "Keywords: Review; Yolo; Object Detection; Public Data Analysis\n",
      "\n",
      "19 used by YOLO V2, it contained 53 convolution layers, so it was called Darknet-53.\n",
      "YOLO V4 style has a significant change, more focus on comparing data, and has a substantial improvement.\n",
      "Peiyuan Jiang et al. / Procedia Computer Science 199 (2022) 1066–1073\n",
      "Author name / Procedia Computer Science 00 (2017) 000–000\n",
      "Because YOLO and YOLO V2 are not effective in detecting small targets, multi-scale detection is added to YOLO V3. YOLO V3 is a well-received master of the previous generations. YOLO V4 sorted out and tried all possible optimizations in the entire process and found the best effect in each permutation and combination. YOLOv4 runs twice faster than EfficientDet with comparable performance. Improves YOLOv3’s AP and FPS by 10% and 12%, respectively [15]. YOLO V5 can flexibly control models from 10+M to 200+M, and its small model is very impressive. The overall network diagrams of YOLO V3 to YOLO V5 are similar, but they also focus on detecting objects of different sizes from three different scales.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, pc_api_key, index_name, model, openai_api_key, search_type: str = \"similarity\", k: int = 5):\n",
    "        self.pc_api_key = pc_api_key\n",
    "        self.index_name=index_name\n",
    "        self.model = model\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.search_type = search_type\n",
    "        self.k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models.base import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "## LLM\n",
    "from langchain.chat_models.base import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# -----------------------------\n",
    "# RAG Chain Class\n",
    "# -----------------------------\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever,\n",
    "                model:str,\n",
    "                pc_api_key:str, \n",
    "                index_name:str, \n",
    "                openai_api_key\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.pc_api_key = pc_api_key\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.index_name=index_name\n",
    "        self.retriever = retriever\n",
    "\n",
    "        self.llm = init_chat_model(model)\n",
    "        self.prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"Answer the question based on the following context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "        \"\"\"\n",
    "        )\n",
    "        self.chain = (\n",
    "            RunnableMap(\n",
    "                {\n",
    "                    \"context\": lambda x: self.retriever.get_relevant_docs(x[\"question\"]),\n",
    "                    \"question\": lambda x: x[\"question\"]\n",
    "                }\n",
    "            )\n",
    "            | self.prompt_template\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def run_query(self, question: str):\n",
    "        query = {\"question\": question}\n",
    "        return self.chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGPipeline(retriever = r1,\n",
    "                model = \"openai:gpt-3.5-turbo\",\n",
    "                pc_api_key = PINECONE_API_KEY, \n",
    "                index_name = OPENAI_API_KEY, \n",
    "                openai_api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreRetriever' object has no attribute 'get_relevant_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is Yolo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mRAGPipeline.run_query\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     46\u001b[39m     query = {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question}\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3243\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3241\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3243\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3244\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3245\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4000\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3995\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3996\u001b[39m         futures = [\n\u001b[32m   3997\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3998\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3999\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   4001\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   4002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3984\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3978\u001b[39m child_config = patch_config(\n\u001b[32m   3979\u001b[39m     config,\n\u001b[32m   3980\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3981\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3982\u001b[39m )\n\u001b[32m   3983\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3984\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5024\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this ``Runnable`` synchronously.\u001b[39;00m\n\u001b[32m   5010\u001b[39m \n\u001b[32m   5011\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5021\u001b[39m \n\u001b[32m   5022\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m5024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5025\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5026\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5030\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5031\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2089\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2085\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2086\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2087\u001b[39m         output = cast(\n\u001b[32m   2088\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2089\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2091\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2097\u001b[39m         )\n\u001b[32m   2098\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2099\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4881\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4879\u001b[39m                 output = chunk\n\u001b[32m   4880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/langchain_core/runnables/config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mRAGPipeline.__init__.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = init_chat_model(model)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.prompt_template = PromptTemplate.from_template(\n\u001b[32m     26\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Answer the question based on the following context:\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    {context}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.chain = (\n\u001b[32m     33\u001b[39m     RunnableMap(\n\u001b[32m     34\u001b[39m         {\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_relevant_docs\u001b[49m(x[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     36\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     37\u001b[39m         }\n\u001b[32m     38\u001b[39m     )\n\u001b[32m     39\u001b[39m     | \u001b[38;5;28mself\u001b[39m.prompt_template\n\u001b[32m     40\u001b[39m     | \u001b[38;5;28mself\u001b[39m.llm\n\u001b[32m     41\u001b[39m     | StrOutputParser()\n\u001b[32m     42\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RAG Pipeline/RAG-Pipeline/myenv/lib/python3.12/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'VectorStoreRetriever' object has no attribute 'get_relevant_docs'"
     ]
    }
   ],
   "source": [
    "rag.run_query(\"What is Yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
